Object Storage và MinIO.
Nói về Object Storage thì sẽ nghĩ ngay tới AWS S3. Đây là một dịch vụ lưu trữ file dạng Object của AWS và cũng là một giao thức truyền file dựa trên HTTP gọi là S3, thực chất nó vẫn là HTTP nhưng payload và API truyền đi chứa định dạng của S3 API. Dựa vào sự phát triển của HTTP thì S3 ra đời với mục đích là file transfer. Tuy nhiên các thể loại truyền thống như FTP, SFTP hay NFS vẫn đang phổ biến hơn vì chúng có khả năng Files Browser Traversal và File Permission. Các giao thức này dựa vào cơ chế "share" để các instance gắn với bộ lưu trữ có thể cùng nhìn thấy nhau, về hiệu suất và tính khả dụng thì giống với mô hình File và Server truyền thống, để ghi file xuống và load file thì chỉ cần FileUpload, ReadFile, WriteFile là được. Tuy nhiên drawback lớn nhất của nó đó là:
Tính ổn định, quá nhiều server hoặc instance mount và truy cập vào cùng thay đổi dữ liệu hoặc thậm chí override quyền có thể dẫn tới các server khác không truy cập được.
Không thể scale out được, về bản chất thì NFS hay FTP là một Storage Pool, nói là Pool nhưng thực chất nó chỉ là một con Server được quản trị ở dưới và gắn vào một cái Disk lớn, Server đó bị làm sao thì tất cả dừng hết. Thậm chí khó setup Master-slave hoặc HA hay replicate dự phòng.
Đa phần đều set permission = root hoặc theo một user id mà yêu cầu tất cả các server phải có cùng User ID đó. Risk về Security hơi cao và khó sửa đổi.
Rủi ro dữ liệu khá lớn, vì là "share" nên khi một app ở một server nào đó xóa file thì các server khác cùng mount vào cũng sẽ bị mất file theo.
Hiệu suất không cao vì toàn bộ server phụ thuộc vào network một chiều tới NFS server và cùng thao tác dữ liệu một lúc.
Kubernetes vẫn phát triển các Storage Class và CSI hỗ trợ việc sử dụng NFS, các Node (server) sẽ cùng chọc vào NFS Server này, Chúng ta có thể tư duy theo kiểu truyền thống và tận dụng NFS Server vì nó là cách nhanh nhất, tối ưu và đơn giản nhất, ưu điểm của NFS vẫn là "share" và là Network FileSystem nên các Pod và ứng dụng của Kubernetes có thể dùng NFS cho mọi loại storage trên Pod, Pod dạng StatefulSet như Database, Deployment sử dụng Storage để lưu Media file, ưu điểm lớn nhất của nó vẫn là "tiện" và dễ dùng vì bản chất là FileSystem.
Lại quay lại vấn đề về hiệu suất và đơn node khi phụ thuộc vào NFS Server. Vậy nếu không sử dụng NFS Server làm Storage Pool thì chúng ta sử dụng cái gì đây ? Khi ở trên Cloud, các Node khác AZ hay thậm chí cùng AZ cũng không thoát được điều này, nếu muốn storage pool chung cho các Node và chia đều ra các AZ thì vẫn phải dùng EFS với base là NFS protocol, các server cùng AZ cũng không thể nào dùng chung một EBS Storage và cũng không có cơ chế hay filesystem cho EBS để dùng chung dữ liệu như kiểu VMFS của VMware được. EFS trên Cloud thậm chí vẫn tồn tại và được sử dụng nhiều. Tuy nhiên nếu muốn thoát ra được NFS, chúng ta sẽ phải tìm một giải pháp mà Cloud Native hay BigData đang hướng tới đó là "lưu trữ phân tán dữ liệu".
Phân tán dữ liệu cơ bản nhất đó là RAID, theo mình nó là nguyên thủy nhất nhưng thực tế thuật toán của RAID đơn giản chỉ là Bit Operator, dựa vào phép XOR, như kiểu A và B sẽ XOR với nhau và sinh ra kết quả lưu trữ và đưa vào C, C sẽ cầm "mảnh" này của A và B. RAID phụ thuộc vào phần cứng khá nhiều và quan trọng nhất đó là nó không thể mở rộng hoặc thay đổi lên RAID cao hơn, nó có thể phân tán dữ liệu các Disk nhưng nó lại không phải phân tán thực sự vì phạm vi của nó chỉ loanh quanh ở trong một server và phụ thuộc vào card RAID controller trên server đó, chúng thậm chí còn bị phụ thuộc vào card raid đó vì nó mà chết thì dữ liệu sẽ bay sạch, kể cả SAN storage cũng không tránh được vấn đề này, tuy nhiên ít nhất nó vẫn còn có 2 SFF controller để HA cho nhau, nhưng vẫn bị giới hạn ở một thiết bị phần cứng. Một loại hình phổ biến khác của ảo hóa và DR (disaster recovery) đó là Stretched Cluster, thực tế nó là RAID giữa 2 AZ hoặc giữa 2 DC, nó là replicate thì đúng hơn. Tuy nhiên replicate cũng là một tính năng trong phân tán dữ liệu và nếu hệ thống đạt được Stretched Cluster thì chúng cũng có thể được coi gần như là "Phân tán dữ liệu". Nếu ai có phản biện về Stretched Cluster thì có thể comment thêm.
Dữ liệu được cho là phân tán khi nó có khả năng copy một file dữ liệu ra nhiều node trên nhiều disk khác nhau và có thể mở rộng bằng cách thêm node hay chậm chí là xóa node (trong trường hợp usable capacity đủ với số node còn lại). Đầu tiên ta phải phân loại Storage thành 2 loại cơ bản nhất đó là Block Storage và Object Storage, Block Storage sẽ ở level thấp hơn và vẫn là các Disk, OSD truyền thống, tuy nhiên chúng sẽ không được cấu hình RAID mà dựa vào Software Storage để tạo các disk thành một Storage Pool. RAID controller sẽ được bỏ đi và metadata + block data sẽ được Software Storage quyết định lưu và sử dụng thuật toán để map chúng lại với nhau và replicate hoặc copy chúng hợp lý tới các Node. Như vậy chúng hoạt động bằng cách mỗi node đều phải có một daemon run và tương tác với các node khác để cấu thành một cụm, hai ứng cử viên nổi bật nhất hiện nay đó là Ceph của Redhat và VMware vSAN. Chúng tạo ra dữ liệu phân tán dạng Block Storage giữa các Node, nhưng vẫn bị một giới hạn đó là chỉ chạy được các Node cùng AZ/DC, vì drawback lớn nhất của chúng đó là tính đồng bộ và time offset, đầu tiên là phải có Disk cache hoặc (bluestore) để cache data trước khi replicate giúp dữ liệu khi ghi vào là có thể ghi tức thời luôn vào một Node trước, sau đó nó mới replicate xuống các Data Disk sau, thứ hai là thời gian phải chuẩn, NTP phải được cấu hình khớp giữa các Node, thứ 3 là phải đồng bộ về tốc độ, cấu hình RAM, CPU, RAM cũng phải lớn để cache layer 1,2 khi kernel copy từ disk cache xuống data node.
Để mở rộng và thoát khỏi phụ thuộc vào một AZ/DC khi triển khai Software Storage, ta lại cấu hình Stretched Cluster giống như VMware vSAN vậy. Tuy nhiên mọi thứ lúc này trở lên quá phức tạp và quá nhiều kỹ thuật, đặc biệt là quá sâu về phần cứng khi triển khai các giải pháp lưu trữ phân tán ở dạng Block Storage.
Block Storage hiệu quả cho việc Pod hay VM dùng chung và migrate dễ dàng giữa các Node và chung dữ liệu, khi triển khai Database thì dựa vào Block Software Storage như Ceph hay vSAN, chúng ta có thể triển khai mô hình gọi là HCI (hypercovered) . Nhưng việc mở rộng và HA sang DC hay AZ khác hơi khó khăn, vì vậy các Database như PostgreSQL hay db MySQL có xu hướng hạn chế việc này và thiết kế database theo hướng Multi-master dạng replicate như PostgreSQL repmgr, EnterpriseDB (PostgreSQL bản trả phí) hoặc Sharding dữ liệu như Vitess giống Youtube hoặc Github, lúc này thì chẳng cần lo về việc phải có storage pool nữa và chúng sẽ tự xử lý xử liệu ở mức Application với nhau mà không còn phụ thuộc vào Storage ở Infrastructure.
Xu hướng đang dần nâng cao và chú trọng hơn vào Application khi sử dụng Softwara Storage mức OS để cấu thành Storage Pool và không cần phụ thuộc vào phần cứng, tiếp đó là sử dụng chính Application để tự phân tán và scale dữ liệu mà cần quan tâm storage hay OS hoặc FileSystem của nó là gì ? Đây mới thực sự là ứng dụng có thể Cloud hóa và Native trên mọi nền tảng, chúng không quan tâm OS là gì ? phần cứng là gì, tự chúng có thể cluster với những node khác, dữ liệu từ Application khá nhỏ và không chung chạn với các Application khác nên chúng cũng không lo về việc bị vấn đề về Network hay khác AZ gì, một bản ghi hoặc một file được ghi lên gần như theo kiểu pipeline vậy. Chúng replicate với nhau cực kỳ nhanh vì chỉ dữ liệu replicate nguyên thủy là bản ghi trực tiếp ở phần App, không cần đi qua nhiều lớp Filesystem. Kể cả các dạng db như NoSQL cũng theo cơ chế này. Chung quy là Application đã "tiến hóa" đến mức tự tổ chức cơ chế phân tán dữ liệu của Application đó mà không còn phụ thuộc vào Storage dưới Infrastructure nữa.
Với các dữ liệu dạng Object Storage thì càng đơn giản hơn khi dữ liệu của chúng là dạng static, không bị thay đổi và realtime nhiều như Database, lưu một lần duy nhất và đọc nhiều lần, với các hệ thống cần lưu trữ media file thì Object Storage là giải pháp tốt nhất hiện nay. Nói về S3 thì nó quá là đơn giản rồi vì việc khởi tạo dịch vụ rất nhanh và sử dụng dễ dàng, khi triển khai hệ thống local hoặc on-prem, nếu sử dụng S3 thì lại vướng issue về đường truyền hay chi phí và việc chạy app dưới on-prem mà muốn lưu trữ Object data dạng phân tán thì sao ?

MinIO là một ứng dụng siêu nhẹ, dễ cấu hình và cực kỳ đơn giản. Việc quan trọng chỉ là đưa ra số liệu và tự tính toán xem có bao nhiêu node, bao nhiêu disk từng node, disk ở đây không phải là Disk ở dưới server mà là disk mà MinIO tự hiểu, nếu triển khai Kubernetes thì chúng ta đều biết chúng có PersistentVolume (PV), PV dạng RWO luôn là block storage, về bản chất là một EBS (AWS) hoặc một Virtual Disk gắn vào Server một cách Dynamic, và một Disk sẽ format toàn bộ thành một Filesystem trên disk đó, nhưng quan trọng nhất là nó dynamic, một Worker Node có 4 pod MinIO thì 4 Pod tương ứng với 4 Disk và 4 disk tự format thành một filesystem cho Pod đó sử dụng. Pod đi node nào thì disk kia theo đến đó, Pod được tạo ra hay xóa đi thì disk cũng theo đó mà xóa đi hay ở di trú đến nơi tương ứng. Vậy là các chúng đã Dynamic hoàn toàn, đặc điểm và lợi thế lớn của phân tán dữ liệu đó là Dynamic và không bị phụ thuộc vào việc server phải có tưng đây node, phải gắn với disk nào hay phải luôn cố định, không thay đổi được. Sau đó ta sẽ "scale" Pod trên các Node và chúng sẽ được gộp thành một Pool Set với nhau. Thành một Object Storage Pool.
Replicate và Erasure Coding. Khi triển khai Object Storage với MinIO, ta đã tự chủ được phần Node và Disk nên không cần quan tâm nó nữa, muốn bao nhiêu thì scale bấy nhiêu. Ta có 2 lựa chọn để triển khai, đó là Replicate, khi mà ta sẽ thiết kế với 2 NodeGroup ở AZ 1 và AZ 2, tuy nhiên lúc này, chúng ta không cần quan tâm về việc phải replicate như nào cho chuẩn vì MinIO đã tự làm điều đó, nó chỉ cần biết là các node với AZ này với số disk như này và ở các node ở AZ kia với số disk như này là có thể tự cân bằng và tính toán được dựa theo số disk/node theo số K/M/N. Tức là ta có thể mất bất cứ Node/disk nào trong khoảng cho phép, mà không cần quan tâm nó nằm ở đâu. Khi mà dữ liệu càng lớn và càng có nhiều node thì triển khai Erasure Coding (EC) lại càng hiệu quả hơn, với >= 8 node thì EC có thể tiết kiệm đến 87% với số node cho phép failover = 2. Lợi thế khi sử dụng MinIO với node lớn đó là có thể sử dụng Erasure Coding để vừa replicate vừa failover tốt hơn mà ít tốn hơn dung lượng để truy trì các "mảnh của file" hơn.
Tuy nhiên để triển khai MinIO hiệu quả và đạt đến cái gọi là "dữ liệu phân tán" thì hạ tầng của chúng ta phải là dạng "Cloud", Node hay Pod hay Disk có thể scale đơn giản hoặc có sẵn Node để chạy bất cứ lúc nào, nhất là Persistent Volume phải là StorageClass với dynamic volume và chỉ cần Persistent Volume ở dạng RWO là được, MinIO sẽ tự xử lý việc cluster thành một Object Storage Pool. MinIO có thể hiệu quả khi kết hợp để xây dựng thành giải pháp lưu trữ dữ liệu tĩnh ở dưới on-prem với môi trường air-gap, no-internet, nhiều server trống, strict về mặt data và internet, việc chạy và run nó rất đơn giản. Đặc biệt là chẳng có file cấu hình gì cả, chỉ phụ thuộc vào file để start minio-server và điền đúng tham số là số node/số disk vào là được. Cấu hình sâu thì có giao diện web hoặc command cấu hình dạng parameter đơn giản, key=value và không có nested key value config. Nói chung nó cực kỳ đơn giản và chỉ cần điền đúng và đủ tham số về số lượng Quorum vào là được.
P/S: Em xin phép thêm ý kiến là ở đây không so sánh hay đưa MinIO lên thành cái tốt nhất hay gì, chỉ đơn giản là nếu muốn build object-storage thì có thể sử dụng MinIO mà không cần dựa vào Ceph hoặc shared block-storage ở dưới, trong hạ tầng on-prem có 2 hoặc nhiều DC mà muốn triển khai Object-Storage, có cần thiết phải build Ceph hay vSAN rồi replicate chúng bằng Streetred Cluster ? MinIO có thể replicate data giữa các Disk ở các Node mà không cần shared block-storage ở dưới nữa, chỉ thuần riêng về Object-storage thì MinIO đủ để làm điều này.
Bài viết hơi dài, mọi người cùng thảo luận thêm nhé 😃